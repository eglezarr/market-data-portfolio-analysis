"""
data_cleaner.py
M√≥dulo para limpieza, homogeneizaci√≥n y validaci√≥n de datos de mercado.

Fase 2 del proyecto: Limpieza y Homogeneizaci√≥n de Datos

Funcionalidades principales:
1. Limpieza de datos (valores faltantes, duplicados, outliers)
2. Homogeneizaci√≥n de formatos entre diferentes fuentes
3. Alineaci√≥n temporal de series de datos
4. Validaci√≥n cruzada entre fuentes (Yahoo vs AlphaVantage)
5. Generaci√≥n de dataset final limpio y validado
"""

import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, List
import logging
from datetime import datetime

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class DataCleaner:
    """
    Limpia, homogeneiza y valida datos de mercado desde m√∫ltiples fuentes.
    
    Estrategia:
    - Yahoo Finance como fuente principal (mejor Adj Close)
    - AlphaVantage como validaci√≥n cruzada
    - Genera reporte de consistencia entre fuentes
    """
    
    def __init__(self, tolerance: float = 0.05):
        """
        Inicializa el limpiador de datos.
        
        Args:
            tolerance: Tolerancia de diferencia permitida entre fuentes (5% por defecto)
        """
        self.tolerance = tolerance
        self.logger = logging.getLogger(self.__class__.__name__)
        self.validation_report = {
            'discrepancies': [],
            'statistics': {},
            'warnings': []
        }
    
    def clean_all_data(
        self,
        raw_data: dict,
        primary_source: str = 'yahoo'
    ) -> Tuple[dict, dict]:
        """
        Limpia todos los datos y genera reporte de validaci√≥n.
        
        Args:
            raw_data: Datos crudos con estructura {source: {stocks: {...}, indices: {...}}}
            primary_source: Fuente principal a usar ('yahoo' o 'alphavantage')
            
        Returns:
            Tuple con:
            - cleaned_data: Datos limpios del source principal
            - validation_report: Reporte de validaci√≥n cruzada
        """
        self.logger.info("="*60)
        self.logger.info("INICIANDO LIMPIEZA Y VALIDACI√ìN DE DATOS")
        self.logger.info("="*60)
        
        # Limpiar datos de cada fuente
        cleaned_sources = {}
        for source, data in raw_data.items():
            self.logger.info(f"\nLimpiando datos de {source.upper()}...")
            cleaned_sources[source] = self._clean_source_data(data, source)
        
        # Validaci√≥n cruzada
        if len(cleaned_sources) > 1:
            self.logger.info("\nRealizando validaci√≥n cruzada entre fuentes...")
            self._cross_validate_sources(cleaned_sources)
            
            # IMPORTANTE: Alinear fechas comunes entre fuentes
            self.logger.info("\nAlineando fechas comunes entre fuentes...")
            cleaned_sources = self._align_common_dates(cleaned_sources)
        
        # Seleccionar fuente principal
        cleaned_data = cleaned_sources.get(primary_source, {})
        
        self.logger.info(f"\n‚úì Limpieza completada. Fuente principal: {primary_source.upper()}")
        
        return cleaned_data, self.validation_report
    
    def _clean_source_data(self, data: dict, source_name: str) -> dict:
        """
        Limpia datos de una fuente espec√≠fica.
        
        Args:
            data: Dict con estructura {stocks: {...}, indices: {...}}
            source_name: Nombre de la fuente
            
        Returns:
            Dict con datos limpios
        """
        cleaned = {'stocks': {}, 'indices': {}}
        
        for data_type in ['stocks', 'indices']:
            if data_type not in data:
                continue
                
            for ticker, df in data[data_type].items():
                self.logger.info(f"  Limpiando {ticker}...")
                
                # Aplicar proceso de limpieza
                df_clean = self._clean_dataframe(df, ticker, source_name)
                
                if df_clean is not None and not df_clean.empty:
                    cleaned[data_type][ticker] = df_clean
                    self.logger.info(f"    ‚úì {ticker}: {len(df_clean)} registros limpios")
                else:
                    self.logger.warning(f"    ‚úó {ticker}: Sin datos despu√©s de limpieza")
        
        return cleaned
    
    def _clean_dataframe(
        self,
        df: pd.DataFrame,
        ticker: str,
        source: str
    ) -> Optional[pd.DataFrame]:
        """
        Limpia un DataFrame individual.
        
        Pasos:
        1. Eliminar duplicados
        2. Manejar valores faltantes
        3. Detectar y tratar outliers
        4. Validar consistencia de datos
        5. Ordenar por fecha
        
        Args:
            df: DataFrame a limpiar
            ticker: S√≠mbolo del ticker
            source: Fuente de datos
            
        Returns:
            DataFrame limpio o None si no se pudo limpiar
        """
        if df is None or df.empty:
            return None
        
        df_clean = df.copy()
        original_len = len(df_clean)
        
        # 1. Eliminar duplicados por fecha
        df_clean = df_clean.drop_duplicates(subset=['Date'], keep='first')
        if len(df_clean) < original_len:
            removed = original_len - len(df_clean)
            self.logger.info(f"      Eliminados {removed} duplicados")
        
        # 2. Ordenar por fecha
        df_clean = df_clean.sort_values('Date').reset_index(drop=True)
        
        # 3. Detectar valores faltantes
        missing_counts = df_clean.isnull().sum()
        if missing_counts.any():
            self.logger.warning(f"      Valores faltantes detectados:")
            for col, count in missing_counts[missing_counts > 0].items():
                self.logger.warning(f"        {col}: {count} valores")
            
            # Interpolar valores faltantes en columnas num√©ricas
            numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
            for col in numeric_cols:
                if col in df_clean.columns and df_clean[col].isnull().any():
                    df_clean[col] = df_clean[col].interpolate(method='linear', limit_direction='both')
        
        # 4. Detectar outliers en volumen y precios
        outliers = self._detect_outliers(df_clean, ticker)
        if outliers:
            self.validation_report['warnings'].append({
                'ticker': ticker,
                'source': source,
                'outliers': outliers
            })
        
        # 5. Validar consistencia OHLC
        df_clean = self._validate_ohlc_consistency(df_clean, ticker)
        
        # 6. Eliminar filas con valores inv√°lidos cr√≠ticos
        df_clean = df_clean.dropna(subset=['Date', 'Close'])
        
        return df_clean
    
    def _detect_outliers(self, df: pd.DataFrame, ticker: str) -> List[dict]:
        """
        Detecta outliers usando el m√©todo IQR (Interquartile Range).
        
        Args:
            df: DataFrame a analizar
            ticker: S√≠mbolo del ticker
            
        Returns:
            Lista de outliers detectados
        """
        outliers = []
        
        # Analizar cambios porcentuales en precio de cierre
        df['pct_change'] = df['Close'].pct_change()
        
        # Calcular IQR
        Q1 = df['pct_change'].quantile(0.25)
        Q3 = df['pct_change'].quantile(0.75)
        IQR = Q3 - Q1
        
        # L√≠mites para outliers
        lower_bound = Q1 - 3 * IQR
        upper_bound = Q3 + 3 * IQR
        
        # Identificar outliers
        outlier_mask = (df['pct_change'] < lower_bound) | (df['pct_change'] > upper_bound)
        
        if outlier_mask.any():
            outlier_dates = df[outlier_mask]['Date'].tolist()
            outlier_changes = df[outlier_mask]['pct_change'].tolist()
            
            for date, change in zip(outlier_dates, outlier_changes):
                if pd.notna(change):
                    outliers.append({
                        'date': str(date),
                        'pct_change': f"{change*100:.2f}%"
                    })
        
        return outliers
    
    def _validate_ohlc_consistency(
        self,
        df: pd.DataFrame,
        ticker: str
    ) -> pd.DataFrame:
        """
        Valida consistencia de datos OHLC (Open, High, Low, Close).
        
        Reglas:
        - High debe ser >= max(Open, Close, Low)
        - Low debe ser <= min(Open, Close, High)
        - Si se detectan inconsistencias, se corrigen o marcan
        
        Args:
            df: DataFrame a validar
            ticker: S√≠mbolo del ticker
            
        Returns:
            DataFrame validado
        """
        df_validated = df.copy()
        
        # Verificar que High >= Low
        invalid_high_low = df_validated['High'] < df_validated['Low']
        if invalid_high_low.any():
            count = invalid_high_low.sum()
            self.logger.warning(f"      {count} registros con High < Low. Corrigiendo...")
            # Intercambiar High y Low
            df_validated.loc[invalid_high_low, ['High', 'Low']] = \
                df_validated.loc[invalid_high_low, ['Low', 'High']].values
        
        # Verificar que High >= Open, Close
        df_validated['High'] = df_validated[['High', 'Open', 'Close']].max(axis=1)
        
        # Verificar que Low <= Open, Close
        df_validated['Low'] = df_validated[['Low', 'Open', 'Close']].min(axis=1)
        
        return df_validated
    
    def _cross_validate_sources(self, cleaned_sources: dict):
        """
        Realiza validaci√≥n cruzada entre diferentes fuentes de datos.
        
        Compara Yahoo Finance vs AlphaVantage para detectar:
        - Diferencias significativas en precios
        - Datos faltantes en una fuente
        - Correlaci√≥n entre fuentes
        
        Args:
            cleaned_sources: Dict con datos limpios de cada fuente
        """
        sources = list(cleaned_sources.keys())
        if len(sources) < 2:
            return
        
        # Comparar fuentes principales (yahoo vs alphavantage)
        yahoo_data = cleaned_sources.get('yahoo', {})
        alpha_data = cleaned_sources.get('alphavantage', {})
        
        if not yahoo_data or not alpha_data:
            return
        
        self.logger.info("\n  Comparando Yahoo Finance vs AlphaVantage...")
        
        # Comparar stocks
        for data_type in ['stocks', 'indices']:
            yahoo_tickers = set(yahoo_data.get(data_type, {}).keys())
            alpha_tickers = set(alpha_data.get(data_type, {}).keys())
            
            common_tickers = yahoo_tickers & alpha_tickers
            
            for ticker in common_tickers:
                yahoo_df = yahoo_data[data_type][ticker]
                alpha_df = alpha_data[data_type][ticker]
                
                comparison = self._compare_dataframes(
                    yahoo_df, alpha_df, ticker, 'yahoo', 'alphavantage'
                )
                
                if comparison:
                    self.validation_report['discrepancies'].append(comparison)
        
        # Calcular estad√≠sticas generales
        self._calculate_validation_statistics()
    
    def _compare_dataframes(
        self,
        df1: pd.DataFrame,
        df2: pd.DataFrame,
        ticker: str,
        source1: str,
        source2: str
    ) -> Optional[dict]:
        """
        Compara dos DataFrames del mismo ticker de diferentes fuentes.
        
        METODOLOG√çA: Compara √∫nicamente retornos diarios
        
        Los retornos diarios (% cambio) son la m√©trica m√°s confiable para validar
        consistencia entre fuentes, ya que no se ven afectados por diferencias en
        ajustes retroactivos o escalas de precios.
        
        Args:
            df1: DataFrame de la primera fuente
            df2: DataFrame de la segunda fuente
            ticker: S√≠mbolo del ticker
            source1: Nombre de la primera fuente
            source2: Nombre de la segunda fuente
            
        Returns:
            Dict con resultados de la comparaci√≥n
        """
        # Alinear por fecha (inner join)
        merged = pd.merge(
            df1[['Date', 'Close']], 
            df2[['Date', 'Close']],
            on='Date',
            how='inner',
            suffixes=(f'_{source1}', f'_{source2}')
        )
        
        if merged.empty:
            self.logger.warning(f"    ‚úó {ticker}: No hay fechas comunes entre fuentes")
            return None
        
        # Calcular retornos diarios para cada fuente
        close1_col = f'Close_{source1}'
        close2_col = f'Close_{source2}'
        
        merged[f'returns_{source1}'] = merged[close1_col].pct_change()
        merged[f'returns_{source2}'] = merged[close2_col].pct_change()
        
        # Eliminar primer valor (NaN por pct_change)
        merged = merged.dropna()
        
        # Diferencia en retornos (en porcentaje)
        merged['diff_returns_pct'] = abs(
            merged[f'returns_{source1}'] - merged[f'returns_{source2}']
        ) * 100  # Convertir a porcentaje
        
        # Estad√≠sticas basadas en RETORNOS
        avg_diff_returns = merged['diff_returns_pct'].mean()
        max_diff_returns = merged['diff_returns_pct'].max()
        correlation_returns = merged[f'returns_{source1}'].corr(merged[f'returns_{source2}'])
        
        comparison_result = {
            'ticker': ticker,
            'common_dates': len(merged),
            'avg_diff_returns_pct': round(avg_diff_returns, 6),
            'max_diff_returns_pct': round(max_diff_returns, 6),
            'correlation_returns': round(correlation_returns, 6),
            'significant_discrepancies': len(merged[merged['diff_returns_pct'] > 0.1]),  # >0.1%
            'status': 'OK' if avg_diff_returns < 0.05 else 'WARNING'  # <0.05% es excelente
        }
        
        # Log resultados
        status_icon = "‚úì" if comparison_result['status'] == 'OK' else "‚ö†"
        self.logger.info(
            f"    {status_icon} {ticker}: Diff={avg_diff_returns:.4f}%, "
            f"Corr={correlation_returns:.4f}, Fechas={len(merged)}"
        )
        
        return comparison_result
    
    def _calculate_validation_statistics(self):
        """Calcula estad√≠sticas generales de la validaci√≥n."""
        if not self.validation_report['discrepancies']:
            return
        
        discrepancies = self.validation_report['discrepancies']
        
        # Solo m√©tricas de retornos
        avg_diffs_returns = [d['avg_diff_returns_pct'] for d in discrepancies]
        correlations_returns = [d['correlation_returns'] for d in discrepancies]
        
        self.validation_report['statistics'] = {
            'total_comparisons': len(discrepancies),
            'avg_diff_returns_overall': round(np.mean(avg_diffs_returns), 6),
            'max_diff_returns_overall': round(np.max([d['max_diff_returns_pct'] for d in discrepancies]), 6),
            'avg_correlation_returns': round(np.mean(correlations_returns), 6),
            'min_correlation_returns': round(np.min(correlations_returns), 6),
            'tickers_with_warnings': sum(1 for d in discrepancies if d['status'] == 'WARNING')
        }
    
    def _align_common_dates(self, cleaned_sources: dict) -> dict:
        """
        Alinea todas las fuentes para que tengan exactamente las mismas fechas.
        
        Soluciona el problema de que diferentes APIs usan diferentes convenciones:
        - Yahoo Finance: [start, end) - NO incluye fecha final
        - AlphaVantage: [start, end] - S√ç incluye fecha final
        
        Aplica inner join sobre fechas para alineaci√≥n perfecta.
        
        Args:
            cleaned_sources: Dict con datos limpios de cada fuente
            
        Returns:
            Dict con datos alineados a fechas comunes
        """
        if len(cleaned_sources) < 2:
            return cleaned_sources
        
        sources = list(cleaned_sources.keys())
        
        # Para cada tipo de dato (stocks, indices)
        for data_type in ['stocks', 'indices']:
            # Tickers comunes a todas las fuentes
            tickers_by_source = [
                set(cleaned_sources[source].get(data_type, {}).keys())
                for source in sources
            ]
            common_tickers = set.intersection(*tickers_by_source) if tickers_by_source else set()
            
            # Para cada ticker com√∫n
            for ticker in common_tickers:
                # Obtener fechas de cada fuente
                dates_by_source = []
                for source in sources:
                    if ticker in cleaned_sources[source].get(data_type, {}):
                        df = cleaned_sources[source][data_type][ticker]
                        dates_by_source.append(set(df['Date'].dt.date))
                
                # Fechas comunes (inner join)
                if dates_by_source:
                    common_dates = set.intersection(*dates_by_source)
                    
                    # Filtrar cada fuente a fechas comunes
                    original_counts = {}
                    for source in sources:
                        if ticker in cleaned_sources[source].get(data_type, {}):
                            df = cleaned_sources[source][data_type][ticker]
                            original_counts[source] = len(df)
                            
                            # Aplicar filtro
                            df_aligned = df[df['Date'].dt.date.isin(common_dates)].copy()
                            df_aligned = df_aligned.sort_values('Date').reset_index(drop=True)
                            cleaned_sources[source][data_type][ticker] = df_aligned
                    
                    # Logging solo si hubo cambios
                    removed_any = any(
                        original_counts[s] > len(cleaned_sources[s][data_type][ticker])
                        for s in sources if s in original_counts
                    )
                    
                    if removed_any:
                        self.logger.info(f"  {ticker}: Alineado a {len(common_dates)} fechas comunes")
                        for source in sources:
                            if source in original_counts:
                                new_count = len(cleaned_sources[source][data_type][ticker])
                                if original_counts[source] != new_count:
                                    removed = original_counts[source] - new_count
                                    self.logger.info(f"    - {source}: {original_counts[source]} ‚Üí {new_count} (-{removed})")
        
        return cleaned_sources
    
    def generate_validation_report(self, save_to_file: bool = True) -> str:
        """
        Genera un reporte detallado de la validaci√≥n en formato Markdown.
        
        Args:
            save_to_file: Si True, guarda el reporte en un archivo
            
        Returns:
            String con el reporte en formato Markdown
        """
        report_lines = []
        
        report_lines.append("# üìä Reporte de Validaci√≥n de Datos")
        report_lines.append(f"\n**Fecha de generaci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"\n**Tolerancia configurada:** {self.tolerance*100}%")
        
        report_lines.append("\n## üìã Metodolog√≠a de Validaci√≥n")
        report_lines.append("\n### Comparaci√≥n: Retornos Diarios")
        report_lines.append("\nPara validar la calidad de los datos, se compararon los **retornos diarios** ")
        report_lines.append("(cambios porcentuales) de Yahoo Finance con los de AlphaVantage.")
        report_lines.append("\n\n**Justificaci√≥n metodol√≥gica:**")
        report_lines.append("\n- Los **precios absolutos** pueden diferir por ajustes retroactivos diferentes")
        report_lines.append("\n- Los **retornos diarios** deben ser id√©nticos si ambas fuentes capturan el mismo mercado")
        report_lines.append("\n- M√°s robusto ante diferencias de escala o metodolog√≠as de ajuste hist√≥rico")
        report_lines.append("\n- Si retorno de Yahoo = +2.5% y retorno de AlphaVantage = +2.5% ‚Üí validaci√≥n perfecta")
        report_lines.append("\n\n**Nota:** Diferencias menores a 0.05% en retornos diarios son excelentes.")
        
        # Estad√≠sticas generales
        if self.validation_report['statistics']:
            stats = self.validation_report['statistics']
            report_lines.append("\n## üìà Estad√≠sticas Generales")
            report_lines.append(f"\n- **Total de comparaciones:** {stats['total_comparisons']}")
            report_lines.append(f"- **Diferencia promedio en retornos:** {stats['avg_diff_returns_overall']:.4f}%")
            report_lines.append(f"- **Diferencia m√°xima en retornos:** {stats['max_diff_returns_overall']:.4f}%")
            report_lines.append(f"- **Correlaci√≥n promedio de retornos:** {stats['avg_correlation_returns']:.6f}")
            report_lines.append(f"- **Correlaci√≥n m√≠nima de retornos:** {stats['min_correlation_returns']:.6f}")
            report_lines.append(f"- **Tickers con advertencias:** {stats['tickers_with_warnings']}")
        
        # Detalles por ticker
        if self.validation_report['discrepancies']:
            report_lines.append("\n## üìã Comparaci√≥n Detallada por Ticker")
            report_lines.append("\n| Ticker | Fechas Comunes | Diff Retornos (%) | Corr Retornos | Estado |")
            report_lines.append("|--------|----------------|-------------------|---------------|--------|")
            
            for disc in self.validation_report['discrepancies']:
                status_icon = "‚úÖ" if disc['status'] == 'OK' else "‚ö†Ô∏è"
                
                report_lines.append(
                    f"| {disc['ticker']} | {disc['common_dates']} | "
                    f"{disc['avg_diff_returns_pct']:.4f} | {disc['correlation_returns']:.6f} | "
                    f"{status_icon} {disc['status']} |"
                )
        
        # Advertencias
        if self.validation_report['warnings']:
            report_lines.append("\n## ‚ö†Ô∏è Advertencias")
            for warning in self.validation_report['warnings']:
                ticker = warning['ticker']
                source = warning['source']
                outliers = warning.get('outliers', [])
                
                if outliers:
                    report_lines.append(f"\n### {ticker} ({source})")
                    report_lines.append(f"\n**Outliers detectados:** {len(outliers)}")
                    if len(outliers) <= 5:
                        for outlier in outliers:
                            report_lines.append(f"- {outlier['date']}: {outlier['pct_change']}")
        
        # Conclusi√≥n
        report_lines.append("\n## ‚úÖ Conclusi√≥n")
        if self.validation_report['statistics']:
            stats = self.validation_report['statistics']
            avg_diff = stats['avg_diff_returns_overall']
            avg_corr = stats['avg_correlation_returns']
            
            # Umbrales ajustados: diff < 0.1% y corr > 0.85 = Buena consistencia
            if avg_diff < 0.01 and avg_corr > 0.99:
                report_lines.append("\n**‚úÖ Excelente consistencia entre fuentes.**")
                report_lines.append("\n\nLa validaci√≥n mediante **retornos diarios** confirma que ambas fuentes ")
                report_lines.append("capturan los mismos movimientos de mercado con precisi√≥n excepcional.")
            elif avg_diff < 0.1 and avg_corr > 0.85:
                report_lines.append("\n**‚úÖ Buena consistencia entre fuentes.**")
                report_lines.append("\n\nLas diferencias son aceptables para an√°lisis estad√≠stico.")
            else:
                report_lines.append("\n**‚ö†Ô∏è Consistencia moderada.**")
                report_lines.append("\n\nSe recomienda revisar las discrepancias detectadas.")
            
            # Justificaci√≥n completa de selecci√≥n de fuente
            report_lines.append(f"\n\n**Fuente seleccionada para an√°lisis:** Yahoo Finance")
            report_lines.append("\n\n**Justificaci√≥n de la selecci√≥n:**")
            report_lines.append("\n\nUna vez validada la calidad y consistencia de los datos mediante la ")
            report_lines.append("comparaci√≥n de retornos diarios, se seleccion√≥ **Yahoo Finance** como ")
            report_lines.append("fuente principal por las siguientes razones:")
            report_lines.append("\n\n1. **Adjusted Close preciso**: Yahoo Finance proporciona precios ajustados ")
            report_lines.append("que corrigen por:")
            report_lines.append("\n   - Dividendos pagados")
            report_lines.append("\n   - Splits y reverse splits de acciones")
            report_lines.append("\n   - Otros eventos corporativos")
            report_lines.append("\n\n2. **An√°lisis de retornos preciso**: El uso de Adj Close es esencial para ")
            report_lines.append("calcular retornos correctos, ya que el Close sin ajustar muestra ca√≠das ")
            report_lines.append("artificiales en fechas de dividendos o splits.")
            report_lines.append("\n\n3. **Cobertura y accesibilidad**: Mayor cobertura hist√≥rica sin l√≠mites de tasa.")
            report_lines.append("\n\n4. **Est√°ndar de industria**: Ampliamente utilizado en an√°lisis cuantitativo.")
            report_lines.append("\n\n**Rol de AlphaVantage:**")
            report_lines.append(f"\n- ‚úÖ Validaci√≥n cruzada exitosa: Correlaci√≥n promedio de retornos de {avg_corr:.6f}")
            report_lines.append(f"\n- ‚úÖ Diferencia promedio de retornos de {avg_diff:.4f}% confirma alta consistencia")
            report_lines.append("\n- ‚úÖ Confirma la captura precisa de movimientos de mercado")
            report_lines.append("\n- ‚úÖ Proporciona redundancia y confianza en el dataset")
        
        report = "\n".join(report_lines)
        
        # Guardar en archivo
        if save_to_file:
            filename = f"VALIDATION_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            self.logger.info(f"\n‚úì Reporte guardado en: {filename}")
        
        return report


def main():
    """Funci√≥n principal para ejecutar la limpieza de datos."""
    from main_extraction import main as extract_data
    
    print("="*60)
    print("FASE 2: LIMPIEZA Y HOMOGENEIZACI√ìN DE DATOS")
    print("="*60)
    
    # Cargar datos crudos
    print("\n1. Cargando datos crudos...")
    raw_data = extract_data()
    
    # Limpiar y validar
    print("\n2. Limpiando y validando datos...")
    cleaner = DataCleaner(tolerance=0.05)
    cleaned_data, validation_report = cleaner.clean_all_data(raw_data, primary_source='yahoo')
    
    # Generar reporte
    print("\n3. Generando reporte de validaci√≥n...")
    report = cleaner.generate_validation_report(save_to_file=True)
    
    print("\n" + "="*60)
    print("‚úì LIMPIEZA COMPLETADA")
    print("="*60)
    print(f"\nDatos limpios disponibles:")
    print(f"  - Acciones: {list(cleaned_data['stocks'].keys())}")
    print(f"  - √çndices: {list(cleaned_data['indices'].keys())}")
    print(f"\nReporte de validaci√≥n generado.")
    
    return cleaned_data, validation_report


if __name__ == "__main__":
    cleaned_data, report = main()